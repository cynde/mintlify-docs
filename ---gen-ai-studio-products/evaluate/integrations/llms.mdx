---
title: "Llms"
---

# LLMs

This section only applies if you want to:

*   Query your LLMs via the Galileo Playground or via promptquality.runs()
    
*   Or leverage any of our the metrics that are powered by OpenAI / Azure models
    

If you have an application or prototype where you're querying a model in code you can integrate Galileo into your code. Jump to [Evaluating and Optimizing Agents, Chains, or multi-stage workflows](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/using-prompt-with-chains-or-multi-step-workflows) to learn more.

Galileo integrates with publicly accessible LLM APIs as well as Open Source LLMs (privately hosted). Before you start using **Evaluate** on your own LLMs, you need to set up your models on the system.

*   Go to the **Galileo Home Page**
    
*   Click on your _**Profile**_ (bottom left)
    
*   Click on _**Integrations**_
    

You can set up and manage all your LLM API and Custom Model integrations from the _Integrations_ page.

_Note:_ These integrations are user-specific to ensure that different users in an organization can use their own API keys when interacting with the LLMs.

### 

[](#public-apis-supported)

Public APIs supported

#### 

[](#openai)

**OpenAI**

We support both the [Chat](https://platform.openai.com/docs/api-reference/chat) and [Completions](https://platform.openai.com/docs/api-reference/completions) APIs from OpenAI, with all of the active models. This can be set up from the Galileo console or from the [Python client](https://promptquality.docs.rungalileo.io/#promptquality.add_openai_integration).

_Note:_ OpenAI Models power a few of Galileo's Guardrail Metrics (e.g. Correctness, Context Adherence, Chunk Attribution, Chunk Utilization, Completeness). To improve your evaluation experience, we recommend setting up this integration even if the model you're prompting or testing is a different one.

#### 

[](#azure-openai)

**Azure OpenAI**

If you use OpenAI models through Azure, you can set up your Azure integration. This can be set up from the Galileo console or from the [Python client](https://promptquality.docs.rungalileo.io/#promptquality.add_azure_integration).

To calculate the <i class="font-italic">Uncertainty</i> metric, we require having<code class="py-[1px] px-1.5 min-w-[1.625rem] inline-flex justify-center items-center ring-1 ring-inset ring-dark/1 bg-dark/[0.06] rounded text-dark/8 dark:ring-light/1 dark:bg-light/1 dark:text-light/7 text-[.875em] leading-[calc(max(1.20em,1.25rem))]">text-curie-001</code> or <code class="py-[1px] px-1.5 min-w-[1.625rem] inline-flex justify-center items-center ring-1 ring-inset ring-dark/1 bg-dark/[0.06] rounded text-dark/8 dark:ring-light/1 dark:bg-light/1 dark:text-light/7 text-[.875em] leading-[calc(max(1.20em,1.25rem))]">text-davinci-003</code>models available in your Azure environment. This is required in order to fetch log probabilities. For Galileo's Guardrail metrics that rely on GPT calls (<i class="font-italic">Factuality</i> and <i class="font-italic">Groundedness</i>), we require using <code class="py-[1px] px-1.5 min-w-[1.625rem] inline-flex justify-center items-center ring-1 ring-inset ring-dark/1 bg-dark/[0.06] rounded text-dark/8 dark:ring-light/1 dark:bg-light/1 dark:text-light/7 text-[.875em] leading-[calc(max(1.20em,1.25rem))]">0613</code> or above versions of <code class="py-[1px] px-1.5 min-w-[1.625rem] inline-flex justify-center items-center ring-1 ring-inset ring-dark/1 bg-dark/[0.06] rounded text-dark/8 dark:ring-light/1 dark:bg-light/1 dark:text-light/7 text-[.875em] leading-[calc(max(1.20em,1.25rem))]">gpt-35-turbo</code> (<a class="underline underline-offset-2 text-primary hover:text-primary-700 transition-colors " href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models">Azure docs</a>).

#### 

[](#google-vertex-ai)

**Google Vertex AI**

For integrating with models served by Google via Vertex AI (like PaLM 2 and Gemini), we recommend setting up a Service Account within your Google Cloud project that has [Vertex AI enabled](/galileo/gen-ai-studio-products/galileo-evaluate/integrations/setting-up-your-llms). This service account requires at minimum [the 'Vertex AI User (roles/aiplatform.user)' role's policies](https://cloud.google.com/vertex-ai/docs/generative-ai/access-control) to be attached.

Once the role is created, create a new key for this service account. The contents of the JSON file provided are what you'll copy over into the Integrations page for Galileo.

#### 

[](#aws-bedrock)

**AWS Bedrock**

Add your AWS Bedrock integration in the Galileo Integrations page. You should see a green light indicating a successful integration. Now, you should see new **Bedrock models** show up in the Prompt Playground.

#### 

[](#aws-sagemaker)

**AWS Sagemaker**

If you're hosting models on AWS Sagemaker, you can query them via Galileo. Set up your AWS Sagemaker integration via the Integrations page.

You'll need to enter your authentication credentials (as an access key &lt;&gt; secret pair or an AWS role that can be assumed) alongwith the AWS region in which your endpoints are hosted. For each endpoint, you can configure the name of the endpoint and an alias alongwith the schema mapping in <a class="underline underline-offset-2 text-primary hover:text-primary-700 transition-colors " href="https://pypi.org/project/dpath/"><code class="py-[1px] px-1.5 min-w-[1.625rem] inline-flex justify-center items-center ring-1 ring-inset ring-dark/1 bg-dark/[0.06] rounded text-dark/8 dark:ring-light/1 dark:bg-light/1 dark:text-light/7 text-[.875em] leading-[calc(max(1.20em,1.25rem))]">dpath notation</code></a>.

Required parameters for each endpoint are:

*   Prompt: To pass the prompt to the payload.
    
*   Response: To parse the response from the response.
    

Optional parameters, which are included in the payload if set, are:

*   Temperature
    
*   Max tokens
    
*   Top K
    
*   Top P
    
*   Frequency penalty
    
*   Presence penalty
    

Check out [this video](https://www.loom.com/share/27a11ceb14b94c84a6248c67515edee8) for step-by-step instructions.

#### 

[](#other-custom-models)

Other Custom Models

If you are prompting via [Langchain](https://python.langchain.com/docs/get_started/introduction), Galileo can use custom models through Langchain the same way you might use OpenAI in Langchain. Check out '[Using Prompt with Chains or multi-step workflows](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/using-prompt-with-chains-or-multi-step-workflows)' for more details on how to integrate Galileo into your Langchain application.

To prompt your custom models through the Galileo UI, they need to be hosted on AWS Sagemaker ([see above](/galileo/gen-ai-studio-products/galileo-evaluate/integrations/setting-up-your-llms#aws-sagemaker)).

[PreviousIntegrations](/galileo/gen-ai-studio-products/galileo-evaluate/integrations)[NextSupported LLMs](/galileo/gen-ai-studio-products/galileo-evaluate/integrations/setting-up-your-llms/supported-llms)

Last updated 16 days ago