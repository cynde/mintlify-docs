---
title: "How To"
---

# How-To

[Create an Evaluation Set](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/creating-an-evaluation-set)[Evaluate and Optimize Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/evaluate-and-optimize-prompts)[Evaluate and Optimize RAG Applications](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/using-prompt-with-rag-or-vector-databases)[Choose your Guardrail Metrics](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/choosing-your-guardrail-metrics)[Enabling Scorers in Runs](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/enabling-scorers-runs)[Identify Hallucinations](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/identifying-hallucinations)[Register Custom Metrics](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/registering-and-using-custom-metrics)[Prompt Management & Storage](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/using-galileo-prompt-as-a-prompt-store)[A/B Compare Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/a-b-compare-prompts)[Experiment with Multiple Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/prompt-sweeps)[Experiment with Multiple Chain Workflows](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/chain-sweeps)[Evaluate with Human Feedback](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/performing-qualitative-evaluations)[Add Tags and Metadata to Prompt Runs](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/adding-tags-to-prompt-runs)[Log Pre-generated Responses in Python](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/logging-pre-generated-responses)[Log Evaluate Runs via REST APIs](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/log-evaluate-runs-via-rest-apis)[Evaluate and Optimize Agents, Chains or multi-step workflows](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/using-prompt-with-chains-or-multi-step-workflows)[Programmatically fetch logged data](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/programmatically-fetching-logged-data)[Collaborate with other personas](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/collaborate-with-other-personas)[Share a project](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/share-a-project)[Export your Evaluation Runs](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/export-your-evaluation-runs)[Understand your metric's values](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/understand-your-metrics-values)[Logging and Comparing against your Expected Answers](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/logging-and-comparing-against-your-expected-answers)[Customize Chainpoll-powered Metrics](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/customize-chainpoll-powered-metrics)

[PreviousPrompt Engineering from a UI](/galileo/gen-ai-studio-products/galileo-evaluate/quickstart/getting-started)[NextCreate an Evaluation Set](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/creating-an-evaluation-set)

Last updated 1 month ago