---
title: "Experiment With Multiple Prompts"
---

# Experiment with Multiple Prompts

Execute prompts in bulk for faster experimentation

In Galileo, you can execute multiple prompt runs using what we call "Prompt Sweeps".

A sweep allows you to execute, in bulk, multiple LLM runs with different combinations of - prompt templates, models, data, and hyperparameters such as temperature. Prompt Sweeps allows you to battle test an LLM completion step in your workflow.

Looking to run "sweeps" on more complex systems, such as Chains, RAG, or Agents? Check out [Chain Sweeps](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/chain-sweeps).

<iframe src="https://cdn.iframe.ly/pl5CFiY" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen="" scrolling="no" allow="encrypted-media *;"></iframe>

Prompt Sweeps for bulk execution and auto tuning

```
import promptquality as pq

import promptquality as pq

import
 promptquality 
as
 pq
from promptquality import Scorers

from promptquality import Scorers

from
 promptquality 
import
 Scorers
from promptquality import SupportedModels

from promptquality import SupportedModels

from
 promptquality 
import
 SupportedModels




models = [

models = [

models 
=
 [
    SupportedModels.text_davinci_3,

    SupportedModels.text_davinci_3,

    SupportedModels
.
text_davinci_3
,
    SupportedModels.chat_gpt_16k,

    SupportedModels.chat_gpt_16k,

    SupportedModels
.
chat_gpt_16k
,
    SupportedModels.gpt_4

    SupportedModels.gpt_4

    SupportedModels
.
gpt_4
]

]

]




templates = [

templates = [

templates 
=
 [
    """ Given the following context, please answer the question. 

    """ Given the following context, please answer the question. 

    
""" Given the following context, please answer the question. 
Context: {context}

Context: {context}

Context: 
{context}
Question: {question}

Question: {question}

Question: 
{question}
Your answer: """,

Your answer: """,

Your answer: """
,
    """ You are a helpful assistant. Given the following context, 

    """ You are a helpful assistant. Given the following context, 

    
""" You are a helpful assistant. Given the following context, 
    please answer the question.

    please answer the question.

    please answer the question.
----

----

----
Context: {context}

Context: {context}

Context: 
{context}
----

----

----
Question: {question}

Question: {question}

Question: 
{question}
----

----

----
Your answer:

Your answer:

Your answer:
""",

""",

"""
,
    """ You are a helpful assistant. Given the following context, 

    """ You are a helpful assistant. Given the following context, 

    
""" You are a helpful assistant. Given the following context, 
    please answer the question. Provide an accurate and factual answer.

    please answer the question. Provide an accurate and factual answer.

    please answer the question. Provide an accurate and factual answer.
----

----

----
Context: {context}

Context: {context}

Context: 
{context}
----

----

----
Question: {question}

Question: {question}

Question: 
{question}
----

----

----
Your answer: """,

Your answer: """,

Your answer: """
,
    """ You are a helpful assistant. Given the following context, 

    """ You are a helpful assistant. Given the following context, 

    
""" You are a helpful assistant. Given the following context, 
    please answer the question. Provide an accurate and factual answer. 

    please answer the question. Provide an accurate and factual answer. 

    please answer the question. Provide an accurate and factual answer. 
    If the question is about science, religion or politics, say "I don't

    If the question is about science, religion or politics, say "I don't

    If the question is about science, religion or politics, say "I don't
     have enough information to answer that question based on the given context."

     have enough information to answer that question based on the given context."

     have enough information to answer that question based on the given context."
----

----

----
Context: {context}

Context: {context}

Context: 
{context}
----

----

----
Question: {question}

Question: {question}

Question: 
{question}
----

----

----
Your answer: """]

Your answer: """]

Your answer: """
]




from promptquality import Scorers

from promptquality import Scorers

from
 promptquality 
import
 Scorers
from promptquality import SupportedModels

from promptquality import SupportedModels

from
 promptquality 
import
 SupportedModels




metrics = [

metrics = [

metrics 
=
 [
    Scorers.context_adherence_plus,

    Scorers.context_adherence_plus,

    Scorers
.
context_adherence_plus
,
    Scorers.context_relevance,

    Scorers.context_relevance,

    Scorers
.
context_relevance
,
    Scorers.correctness,

    Scorers.correctness,

    Scorers
.
correctness
,
    Scorers.latency,

    Scorers.latency,

    Scorers
.
latency
,
    Scorers.sexist,

    Scorers.sexist,

    Scorers
.
sexist
,
    Scorers.pii

    Scorers.pii

    Scorers
.
pii
    # Uncertainty, BLEU and ROUGE are automatically included

    # Uncertainty, BLEU and ROUGE are automatically included

    
# Uncertainty, BLEU and ROUGE are automatically included
]

]

]




pq.run_sweep(project_name='my_project_name',

pq.run_sweep(project_name='my_project_name',

pq
.
run_sweep
(project_name
=
'my_project_name'
,
             templates=templates,

             templates=templates,

             templates
=
templates,
             dataset='my_dataset.csv',

             dataset='my_dataset.csv',

             dataset
=
'my_dataset.csv'
,
             scorers=metrics,

             scorers=metrics,

             scorers
=
metrics,
             model_aliases=models,

             model_aliases=models,

             model_aliases
=
models,
             execute=True)
             execute=True)
             execute
=
True
)
```

See the [PromptQuality Python Library Docs](https://promptquality.docs.rungalileo.io/) for more information.

[PreviousA/B Compare Prompts](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/a-b-compare-prompts)[NextExperiment with Multiple Chain Workflows](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/chain-sweeps)

Last updated 3 months ago