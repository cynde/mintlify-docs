---
title: "Identify Hallucinations"
---

# Identify Hallucinations

How to use Galileo Evaluate to find Hallucinations

_Hallucination_ can have many definitions. In the realm of closed-book question answering, hallucinations may pertain to _Correctness_ (i.e. is my output factually consistent). In open-book scenarios, hallucinations might be linked to the grounding of information or _Adherence_ (i.e., whether the facts presented in my response "**adhere to**" or "**are grounded in**" the documents I supplied). Hallucinations happen when models produce responses outside of the context being forced upon the model via the prompt_._ Galileo aims to help you identify and solve these hallucinations.

## 

[](#guardrail-metrics)

**Guardrail Metrics**

Galileo's Guardrail Metrics are built to help you shed light on where and why the model produces an undesirable output.

### 

[](#uncertainty)

Uncertainty

Uncertainty measures the model's certainty in its generated tokens. Because uncertainty works at the token level, it can be a great way of identifying _where_ in the response the model started hallucinating.

When prompted for citations of papers on the phenomenon of "Human & AI collaboration", OpenAI's ChatGPT responds with this:

![](https://docs.rungalileo.io/~gitbook/image?url=https%3A%2F%2F1924900477-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252F4jVWiQpRqmi04OnqZmn2%252Fuploads%252FSeb9KAbm2remoVbQmVm2%252Fimage.png%3Falt%3Dmedia%26token%3Dd1efa6c8-ffa3-4307-b22d-8453330a6cf8&width=768&dpr=4&quality=100&sign=77b7d626&sv=1)

ChatGPT's response to a prompt asking for citations. Low, Medium and High Uncertainty is colored in Green, Yellow and Red.

A quick Google Search reveals that the cited paper doesn't exist. The arxiv link takes us to a completely [unrelated paper](https://arxiv.org/abs/1903.03097).

While not every 'high uncertainty' token (shown in red) will contain hallucinations, and not every hallucination will contain high uncertainty tokens, we've seen a strong correlation between the two. Looking for _Uncertainty_ is usually a good first step in identifying hallucinations.

_Note:_ Uncertainty requires log probabilities and only works for certain models for now.

### 

[](#context-adherence)

Context Adherence

Context Adherence measures whether your model's response was purely based on the context provided, i.e. the response didn't state any facts not contained in the context provided. For RAG users, _Context Adherence_ is a measurement of hallucinations.

If a response is _grounded_ in the context (i.e. it has a value of 1 or close to 1), it only contains information given in the context. If a response is _not grounded_ (i.e. it has a value of 0 or close to 0), it's likely to contain facts not included in the context provided to the model.

![](https://docs.rungalileo.io/~gitbook/image?url=https%3A%2F%2F1924900477-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252F4jVWiQpRqmi04OnqZmn2%252Fuploads%252FWA8TcTnv4JW3oLPir78L%252Fimage.png%3Falt%3Dmedia%26token%3D77e33226-dfd6-4062-8b89-d4f2aede05da&width=768&dpr=4&quality=100&sign=2bff4b61&sv=1)

Explanation provided by the Chainpoll methodology for a hallucination metric called Context Adherence, ideally suited for RAG systems

### 

[](#correctness)

Correctness

_Correctness_ measures whether the facts stated in the response are based on real facts. This metric requires additional LLM calls.

If the response is _factually consistent_ (value close to 1), the information is likely be correct. We use our proprietary **ChainPoll Technique** ([Research Paper Link](https://arxiv.org/abs/2310.18344)) using a combination of Chain-of-Thought prompting and Ensembling techniques to provide the user with a 0-1 score and an explanation to the Hallucination. The explanation why something was deemed incorrect or not can be seen upon hovering over the metric value.

_Note:_ Because **correctness** relies on external Large Language Models and their knowledge base, its results are only as good as those models' knowledge base.

![](https://docs.rungalileo.io/~gitbook/image?url=https%3A%2F%2F1924900477-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252F4jVWiQpRqmi04OnqZmn2%252Fuploads%252FU77Ud9VDKCsmUCCuRoP4%252FChainpoll.png%3Falt%3Dmedia%26token%3D92526929-a6e1-4592-a756-26d75b2fda63&width=768&dpr=4&quality=100&sign=21b3617b&sv=1)

ChainPoll Workflow

## 

[](#what-if-i-have-my-own-definition-of-hallucination)

What if I have my own definition of Hallucination?

Enterprise users often have their own unique interpretations of what constitutes hallucinations. Galileo supports [_Custom Metrics_](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/choosing-your-guardrail-metrics#custom-metrics) and incorporates [_Human Feedback and Ratings_](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/performing-qualitative-evaluations), empowering you to tailor Galileo Prompt to align with your specific needs and the particular definition of hallucinations relevant to your use case.

With Galileo's Experimentation and Evaluation features, you can systematically iterate on your prompts and models, ensuring a rigorous and scientific approach to improving the quality of responses and addressing hallucination-related challenges.

[PreviousEnabling Scorers in Runs](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/enabling-scorers-runs)[NextRegister Custom Metrics](/galileo/gen-ai-studio-products/galileo-evaluate/how-to/registering-and-using-custom-metrics)

Last updated 3 months ago