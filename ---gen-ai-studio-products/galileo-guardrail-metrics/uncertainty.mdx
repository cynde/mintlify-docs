---
title: "Uncertainty"
---

# Uncertainty

Understand Galileo's Uncertainty Metric

<iframe src="https://cdn.iframe.ly/QmLMmBg" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen="" scrolling="no" allow="encrypted-media *;"></iframe>

_**Definition**__:_ Measures how much the model is deciding randomly between multiple ways of continuing the output. _Uncertainty_ is measured at both the token level and the response level. Higher uncertainty means the model is less certain.

_**Calculation**__: Uncertainty_ at the token level tells us how confident the model is of the next token given the preceding tokens. _Uncertainty_ at the response level is simply the maximum token-level _Uncertainty,_ over all the tokens in the model's response. It is calculated using OpenAI's Davinci models.

_**What to do when uncertainty is low?**_

Our research has found high uncertainty scores correlate with hallucinations, made up facts, and citations. Looking at highly uncertain responses can flag areas where your model is struggling.

[PreviousChunk Utilization Plus](/galileo/gen-ai-studio-products/guardrail-store/chunk-utilization/chunk-utilization)[NextPrompt Perplexity](/galileo/gen-ai-studio-products/guardrail-store/prompt-perplexity)

Last updated 2 months ago