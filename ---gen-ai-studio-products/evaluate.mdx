---
title: "Evaluate"
---

# Evaluate

_Galileo Evaluate_ is a powerful bench for rapid, collaborative experimentation and evaluation of your LLM applications.

<iframe src="https://cdn.iframe.ly/zLJWK10" style="top: 0; left: 0; width: 100%; height: 100%; position: absolute; border: 0;" allowfullscreen="" scrolling="no" allow="encrypted-media *;"></iframe>

## 

[](#core-features)

Core features

*   **Tracing and Visualizations** - Track the end-to-end execution of your queries. See what happened along the way and where things went wrong.
    
*   **State-of-the-art Metrics -** Combine our research-backed Guardrail Metrics with your own Custom Metrics to evaluate your system.
    
*   **Experiment Management** - Track all your experiments in one place. Find the best configuration for your system.
    

![](https://docs.rungalileo.io/~gitbook/image?url=https%3A%2F%2F1924900477-files.gitbook.io%2F%7E%2Ffiles%2Fv0%2Fb%2Fgitbook-x-prod.appspot.com%2Fo%2Fspaces%252F4jVWiQpRqmi04OnqZmn2%252Fuploads%252FmdoRaeTIctFT6ulxQSf6%252Fimage.png%3Falt%3Dmedia%26token%3Df0a8e7a9-c38f-422f-af40-d7570a6eb0b9&width=768&dpr=4&quality=100&sign=6fc48079&sv=1)

Visualizing a RAG Workflow built using Langchain

## 

[](#getting-started)

Getting Started

[Quickstart](/galileo/gen-ai-studio-products/galileo-evaluate/quickstart)

[PreviousWhat is Galileo?](/galileo)[NextQuickstart](/galileo/gen-ai-studio-products/galileo-evaluate/quickstart)

Last updated 2 months ago